{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Submitted by: Satyajit Pattnaik**","metadata":{"id":"_jIUygegq6GX"}},{"cell_type":"markdown","source":"**Problem statement:** To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.","metadata":{"id":"yDriIbfa5lwD"}},{"cell_type":"markdown","source":"### Importing all the important libraries","metadata":{"id":"JfcpIXQZN2Rh"}},{"cell_type":"code","source":"import pathlib\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport PIL\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","metadata":{"id":"WC8xCQuELWms","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Data","metadata":{"id":"UQ3J5ouueD-s"}},{"cell_type":"markdown","source":"This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively.","metadata":{"id":"RpUsRQwOOL72"}},{"cell_type":"code","source":"# Defining the path for train and test images\ndata_dir_train = pathlib.Path(\"../input/skin-cancer/Skin cancer ISIC The International Skin Imaging Collaboration/Train\")\ndata_dir_test = pathlib.Path('../input/skin-cancer/Skin cancer ISIC The International Skin Imaging Collaboration/Test')","metadata":{"id":"D57L-ovIKtI4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\nprint(image_count_train)\nimage_count_test = len(list(data_dir_test.glob('*/*.jpg')))\nprint(image_count_test)","metadata":{"id":"DqksN1w5Fu-N","outputId":"f38312f6-534c-45b1-aab5-62eb6b3f2a6c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load using keras.preprocessing\n\nLet's load these images off disk using the helpful image_dataset_from_directory utility.","metadata":{"id":"O8HkfW3jPJun"}},{"cell_type":"markdown","source":"### Create a dataset\n\nDefine some parameters for the loader:","metadata":{"id":"cDBKZG3jPcMc"}},{"cell_type":"code","source":"batch_size = 32\nimg_height = 180\nimg_width = 180","metadata":{"id":"VLfcXcZ9LjGv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use 80% of the images for training, and 20% for validation.","metadata":{"id":"Y5f5y43GPog1"}},{"cell_type":"code","source":"## Write your train dataset here\n## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir_train,\n    seed=123,\n    validation_split= 0.2,\n    subset= 'training',\n    image_size=(img_height,img_width),\n    batch_size = batch_size\n)\n","metadata":{"id":"G1BWmDzr7w-5","outputId":"a3115a3d-5c29-4c00-caed-347619c88e67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Write your validation dataset here\n## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir_train,\n    seed=123,\n    validation_split= 0.2,\n    subset= 'validation',\n    image_size=(img_height,img_width),\n    batch_size = batch_size\n)","metadata":{"id":"LYch6-SR-i2g","outputId":"1b178a52-1fcf-4b40-c5f8-306e42c3f5aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List out all the classes of skin cancer and store them in a list. \n# You can find the class names in the class_names attribute on these datasets. \n# These correspond to the directory names in alphabetical order.\nclass_names = train_ds.class_names\nprint(class_names)","metadata":{"id":"Bk0RV7G7-nad","outputId":"30fb3336-790b-4144-e61b-5c201940ccfb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize the data\n#### Below is the code to visualize one instance of all the nine classes present in the dataset","metadata":{"id":"jbsm5oYiQH_b"}},{"cell_type":"code","source":"import matplotlib.image as mpimg\nplt.figure(figsize=(10,10))\nfor i in range(9): \n  plt.subplot(3, 3, i + 1)\n  image = mpimg.imread(str(list(data_dir_train.glob(class_names[i]+'/*.jpg'))[1]))\n  plt.title(class_names[i])\n  plt.imshow(image)","metadata":{"id":"juOo-SOHcvRn","outputId":"10913d93-94de-46fc-a1a1-7b237e721d15","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.","metadata":{"id":"8cAZPYaeQjQy"}},{"cell_type":"markdown","source":"`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\n\n`Dataset.prefetch()` overlaps data preprocessing and model execution while training.","metadata":{"id":"jzVXBHiyQ7_I"}},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"id":"7wZlKRBEGNtU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the model\n#### Below is the first base model`we have created:","metadata":{"id":"1JEAF6-sRyz8"}},{"cell_type":"code","source":"from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nnum_classes = 9\nmodel = Sequential([\n                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n])\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (180, 180, 32)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation = \"softmax\"))","metadata":{"id":"ync9xoW7GZgn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compile the model\nChoose an appropirate optimiser and loss function for model training ","metadata":{"id":"SDKzJmHwSCtt"}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop","metadata":{"id":"apWaOwEHE6dz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As it is a multi class classification, we are using SparseCategoricalCrossEntropy as the loss function, and we are using adam optimizer as a hit & trial, further we can perform hyper parameter optimization and change the optimizer accordingly","metadata":{"id":"lWLOuGZc0gSk"}},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"id":"XB8wKtiPGe1j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the summary of all layers\nmodel.summary()","metadata":{"id":"_ZGWN4MZGhtJ","outputId":"94fcee4e-f88f-4549-e3c6-9793729e63f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model","metadata":{"id":"ljD_83rwSl5O"}},{"cell_type":"code","source":"epochs=30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","metadata":{"id":"Kkfw2rJXGlYC","outputId":"be32bcfa-aad3-42f3-aa11-826635567c13","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing training results","metadata":{"id":"w3679V8OShSE"}},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"id":"R1xkgk5nGubz","outputId":"667d65a5-a7ad-49a6-8f34-1585f3ebf556","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Findings on the first base model\n\n\n*   Initial findings: The model is overfitting because overfitting is calculated w.r.t loss, and we can also see difference in loss functions in training & test around the 19-20th epoch \n*   The accuracy is just around 50-60% because there are enough features to remember the pattern, and the neural network is very young (just 20 epochs), so the learning has just started\n*   But again, it's too early to comment on the overfitting & underfitting debate\n\n","metadata":{"id":"JvPphJYuSZhK"}},{"cell_type":"code","source":"\ndata_aug = keras.Sequential([\n                             layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\",input_shape=(img_height,img_width,3)),\n                             layers.experimental.preprocessing.RandomRotation(0.2, fill_mode='reflect'),\n                             layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3), fill_mode='reflect')\n])","metadata":{"id":"22hljAl6GykA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize how your augmentation strategy works for one instance of training image.\nplt.figure(figsize=(12, 12))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(data_aug(images)[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","metadata":{"id":"Y37fGIGm-d43","outputId":"2f2e2399-9f74-41c9-b8e0-61303ad5dd90","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Creation, compilation and training the model\n","metadata":{"id":"XhKDHlUdTuSX"}},{"cell_type":"code","source":"## You can use Dropout layer if there is an evidence of overfitting in your findings\n\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nnum_classes = 9\nmodel = Sequential([ data_aug,\n                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n      \n])\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (180, 180, 32)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation = \"softmax\"))\n","metadata":{"id":"W3V4l-O9G3dM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compiling the model","metadata":{"id":"FfUWFp96UIAN"}},{"cell_type":"code","source":"## Your code goes here\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"id":"_-7yTm8IG8zR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the model","metadata":{"id":"kC-D_RWOURp6"}},{"cell_type":"code","source":"## Your code goes here, note: train your model for 100 epochs\nepochs=30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","metadata":{"id":"UcPfkUASHBf9","outputId":"53606113-66f3-4f90-a546-f8358269c050","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the results","metadata":{"id":"IhNOKtSyUYzC"}},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"id":"vjN_F4QxHIsh","outputId":"bb0c2643-d991-4f55-8d85-94f6c417c0cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Findings:\n\n\n*   We don't see much improvements with respect to accuracy from the base model, but we can definitely see the overfitting issue fading away due to data augmentation\n*   But again, judging based on just 20 epochs won't give us proper conclusions\n\n","metadata":{"id":"0-AUR_b7UcaK"}},{"cell_type":"markdown","source":"## Finding the distribution of classes in the training dataset.\n","metadata":{"id":"7TdDi4u-VTkW"}},{"cell_type":"code","source":"path_list=[]\nlesion_list=[]\nfor i in class_names:\n      \n    for j in data_dir_train.glob(i+'/*.jpg'):\n        path_list.append(str(j))\n        lesion_list.append(i)\ndataframe_dict_original = dict(zip(path_list, lesion_list))\noriginal_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\noriginal_df","metadata":{"id":"U6y8ud0aMx8U","outputId":"2e8815d2-942e-40f5-b8b4-3984cfb5dba9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe_dict_original = dict(zip(path_list, lesion_list))\noriginal_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\noriginal_df","metadata":{"id":"kAJyr07UMrZe","outputId":"0ea22d91-1c14-4e4b-afbf-8425d8843db1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=[]\nfor i in class_names:\n    count.append(len(list(data_dir_train.glob(i+'/*.jpg'))))\nplt.figure(figsize=(25,10))\nplt.bar(class_names,count)","metadata":{"id":"MYUCnuWWkPeI","outputId":"de5bad4b-d6f9-4563-862b-ac7c19c9ab60","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Findings\n\n#### - Seborrheic keratosis is having the lowest distribution\n#### - Pigmented Benign keratosis is having the highest distribution of data\n","metadata":{"id":"4csQL1dvO0b2"}},{"cell_type":"markdown","source":"#### Visualize the model results","metadata":{"id":"iuvfCTsBWLMp"}},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"id":"lCTXwfkTdRW1","outputId":"cf25f2bf-d596-4968-a11c-0239c5747068","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"VH_Eud6eFIe8"},"execution_count":null,"outputs":[]}]}